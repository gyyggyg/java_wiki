"""LLM è°ƒç”¨æ¥å£å°è£…ï¼ˆé›†æˆ Mermaid éªŒè¯ï¼‰"""
import os
import logging
from typing import Dict, Any, Optional, List
import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage

from .mermaid_validator import MermaidValidator

logger = logging.getLogger(__name__)


# ä¼˜å…ˆä»é¡¹ç›®æ ¹æˆ–å½“å‰ç›®å½•è‡ªåŠ¨åŠ è½½ .envï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv, find_dotenv
    _DOTENV_PATH = find_dotenv(usecwd=True)
    if _DOTENV_PATH:
        load_dotenv(_DOTENV_PATH, override=False)
        logger.info(f"ğŸ“„ å·²åŠ è½½ .env: {_DOTENV_PATH}")
    else:
        logger.debug("æœªå‘ç° .env æ–‡ä»¶ï¼Œè·³è¿‡åŠ è½½")
except Exception as _e:
    logger.debug(f"åŠ è½½ .env å¤±è´¥: {_e}")


class LLMInterface:
    """LLM è°ƒç”¨æ¥å£ï¼Œå°è£…å¸¸ç”¨çš„ LLM æ“ä½œ"""
    
    def __init__(
        self,
        model: str = None,
        temperature: float = None,
        api_key: str = None,
        enable_mermaid_validation: bool = True
    ):
        """
        åˆå§‹åŒ– LLM æ¥å£
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡æˆ– .env è¯»å–
            temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡æˆ– .env è¯»å–
            api_key: API Keyï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡æˆ– .env è¯»å–
            enable_mermaid_validation: æ˜¯å¦å¯ç”¨ Mermaid è‡ªåŠ¨éªŒè¯å’Œä¿®å¤
        """
        # ä»…ä»ç¯å¢ƒå˜é‡/.envè¯»å–
        self.model = model or os.getenv("LLM_MODEL", "gpt-4.1")
        self.temperature = temperature or float(os.getenv("LLM_TEMPERATURE", "0.3"))
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.enable_mermaid_validation = enable_mermaid_validation
        
        # åˆå§‹åŒ– Mermaid éªŒè¯å™¨
        if self.enable_mermaid_validation:
            self.mermaid_validator = MermaidValidator()
            logger.info("ğŸ”§ å·²å¯ç”¨ Mermaid è‡ªåŠ¨éªŒè¯å’Œä¿®å¤")
        
        # è‹¥ç¯å¢ƒå˜é‡å­˜åœ¨ä½†ä¸ºç©ºï¼Œå°è¯•å¼ºåˆ¶è¦†ç›–åŠ è½½ .env
        if not self.api_key:
            try:
                from dotenv import load_dotenv, find_dotenv
                _p = find_dotenv(usecwd=True)
                if _p:
                    load_dotenv(_p, override=True)
                    self.api_key = os.getenv("OPENAI_API_KEY")
            except Exception:
                pass
        
        if not self.api_key:
            raise RuntimeError(
                "æœªæ£€æµ‹åˆ° OPENAI_API_KEYã€‚è¯·åœ¨ç¯å¢ƒå˜é‡æˆ– .env ä¸­é…ç½® OPENAI_API_KEY åé‡è¯•ã€‚"
            )
        
        self.llm = ChatOpenAI(
            model=self.model,
            temperature=self.temperature,
            api_key=self.api_key
        )
        logger.info(f"ğŸ¤– åˆå§‹åŒ– LLM: {self.model} (æ¸©åº¦={self.temperature})")
    
    # ä¸å†æ”¯æŒä» config.json è¯»å–é…ç½®
    
    def invoke(self, prompt: str, system_message: str = None) -> str:
        """
        è°ƒç”¨ LLM
        
        Args:
            prompt: æç¤ºè¯
            system_message: å¯é€‰çš„ç³»ç»Ÿæ¶ˆæ¯
            
        Returns:
            LLM å“åº”æ–‡æœ¬
        """
        messages = []
        
        if system_message:
            messages.append(SystemMessage(content=system_message))
        
        messages.append(HumanMessage(content=prompt))
        
        logger.debug(f"ğŸ“¤ è°ƒç”¨ LLM")
        response = self.llm.invoke(messages)
        
        return response.content
    
    def invoke_with_template(
        self, 
        template: str, 
        variables: Dict[str, Any],
        system_message: str = None,
        expected_diagram_type: str = None
    ) -> str:
        """
        ä½¿ç”¨æ¨¡æ¿è°ƒç”¨ LLMï¼Œè‡ªåŠ¨éªŒè¯å’Œä¿®å¤ Mermaid ä»£ç 
        
        Args:
            template: æç¤ºè¯æ¨¡æ¿
            variables: æ¨¡æ¿å˜é‡å­—å…¸
            system_message: å¯é€‰çš„ç³»ç»Ÿæ¶ˆæ¯
            expected_diagram_type: æœŸæœ›çš„å›¾è¡¨ç±»å‹ï¼ˆç”¨äºéªŒè¯ï¼‰
            
        Returns:
            LLM å“åº”æ–‡æœ¬ï¼ˆå·²éªŒè¯å’Œä¿®å¤çš„ Mermaid ä»£ç ï¼‰
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if system_message:
            messages.append(SystemMessage(content=system_message))
        
        # æ ¼å¼åŒ–ç”¨æˆ·æ¶ˆæ¯
        prompt = ChatPromptTemplate.from_template(template)
        formatted_messages = prompt.format_messages(**variables)
        messages.extend(formatted_messages)
        
        # è°ƒç”¨ LLM
        logger.debug(f"ğŸ“¤ è°ƒç”¨ LLMï¼Œå˜é‡: {list(variables.keys())}")
        response = self.llm.invoke(messages)
        
        content = response.content
        
        # å¦‚æœå¯ç”¨äº† Mermaid éªŒè¯ï¼Œè¿›è¡ŒéªŒè¯å’Œä¿®å¤
        if self.enable_mermaid_validation and expected_diagram_type:
            content = self._validate_and_fix_mermaid(content, expected_diagram_type)
        
        return content
    
    async def ainvoke_with_template(
        self,
        template: str,
        variables: Dict[str, Any],
        system_message: str = None
    ) -> str:
        """
        ä½¿ç”¨æ¨¡æ¿å¼‚æ­¥è°ƒç”¨ LLMï¼ˆå•æ¡ï¼‰
        """
        messages = []
        if system_message:
            messages.append(SystemMessage(content=system_message))
        prompt = ChatPromptTemplate.from_template(template)
        formatted_messages = prompt.format_messages(**variables)
        messages.extend(formatted_messages)
        response = await self.llm.ainvoke(messages)
        return response.content

    async def abatch_with_template(
        self,
        template: str,
        variables_list: List[Dict[str, Any]],
        system_message: str = None,
        concurrency: int = 5,
        expected_diagram_type: str = None
    ) -> List[str]:
        """
        ä½¿ç”¨æ¨¡æ¿æ‰¹é‡å¼‚æ­¥è°ƒç”¨ LLMï¼Œå¸¦å¹¶å‘åº¦é™åˆ¶ã€‚
        """
        semaphore = asyncio.Semaphore(concurrency)

        async def _worker(vars_: Dict[str, Any]) -> str:
            async with semaphore:
                content = await self.ainvoke_with_template(
                    template=template,
                    variables=vars_,
                    system_message=system_message,
                )
                
                # å¦‚æœå¯ç”¨äº† Mermaid éªŒè¯ï¼Œè¿›è¡ŒéªŒè¯å’Œä¿®å¤
                if self.enable_mermaid_validation and expected_diagram_type:
                    content = self._validate_and_fix_mermaid(content, expected_diagram_type)
                
                return content

        tasks = [asyncio.create_task(_worker(v)) for v in variables_list]
        return await asyncio.gather(*tasks)
    
    @staticmethod
    def clean_mermaid_code(code: str) -> str:
        """
        ç®€å•æ¸…ç† Mermaid ä»£ç ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
        
        æ³¨æ„ï¼šæ¨èä½¿ç”¨ MermaidValidator è¿›è¡Œå®Œæ•´çš„éªŒè¯å’Œä¿®å¤
        
        Args:
            code: åŸå§‹ä»£ç 
            
        Returns:
            æ¸…ç†åçš„ä»£ç 
        """
        import re
        
        if not code:
            return code
        
        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        code = code.strip()
        code = re.sub(r"^```(?:mermaid)?\s*\n?", "", code.strip())
        code = re.sub(r"\n?```\s*$", "", code.strip())
        
        return code.strip()
    
    def _validate_and_fix_mermaid(self, code: str, diagram_type: str = None) -> str:
        """
        éªŒè¯å¹¶ä¿®å¤ Mermaid ä»£ç 
        
        Args:
            code: åŸå§‹ Mermaid ä»£ç 
            diagram_type: æœŸæœ›çš„å›¾è¡¨ç±»å‹
            
        Returns:
            ä¿®å¤åçš„ Mermaid ä»£ç 
        """
        # ä½¿ç”¨éªŒè¯å™¨è¿›è¡ŒéªŒè¯å’Œä¿®å¤ï¼ˆå†…éƒ¨å·²åŒ…å«åŸºæœ¬æ¸…ç†ï¼‰
        is_valid, fixed_code, warnings = self.mermaid_validator.validate_and_fix(code, diagram_type)
        
        if warnings:
            logger.warning(f"Mermaid ä»£ç ä¿®å¤: {len(warnings)} ä¸ªé—®é¢˜")
            for warning in warnings[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªè­¦å‘Š
                logger.debug(f"  - {warning}")
            if len(warnings) > 5:
                logger.debug(f"  ... è¿˜æœ‰ {len(warnings) - 5} ä¸ªè­¦å‘Š")
        
        if not is_valid:
            logger.error("âš ï¸ Mermaid ä»£ç éªŒè¯å¤±è´¥ï¼Œä½†å·²å°½åŠ›ä¿®å¤")
        
        return fixed_code